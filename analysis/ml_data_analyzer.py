"""
Î®∏Ïã†Îü¨Îãù Îç∞Ïù¥ÌÑ∞ Î∂ÑÏÑù ÎèÑÍµ¨
ÏàòÏßëÎêú Ïã†Ìò∏/Îß§Ïàò Îç∞Ïù¥ÌÑ∞Î•º Î∂ÑÏÑùÌïòÏó¨ Ìå®ÌÑ¥Í≥º Ïù∏ÏÇ¨Ïù¥Ìä∏ ÎèÑÏ∂ú
"""
import sqlite3
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
import json
from pathlib import Path

# ÌïúÍ∏Ä Ìè∞Ìä∏ ÏÑ§Ï†ï
plt.rcParams['font.family'] = 'Malgun Gothic'
plt.rcParams['axes.unicode_minus'] = False

class MLDataAnalyzer:
    """üí° Î®∏Ïã†Îü¨Îãù Îç∞Ïù¥ÌÑ∞ Î∂ÑÏÑùÍ∏∞"""

    def __init__(self, db_path: str = "data/ml_training_data.db"):
        """Ï¥àÍ∏∞Ìôî"""
        self.db_path = Path(db_path)
        if not self.db_path.exists():
            raise FileNotFoundError(f"Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ ÌååÏùºÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§: {self.db_path}")
        
        print(f"üîç ML Îç∞Ïù¥ÌÑ∞ Î∂ÑÏÑùÍ∏∞ Ï¥àÍ∏∞Ìôî: {self.db_path}")

    def get_basic_stats(self) -> Dict:
        """üìä Í∏∞Î≥∏ ÌÜµÍ≥Ñ Ï†ïÎ≥¥"""
        with sqlite3.connect(str(self.db_path)) as conn:
            stats = {}
            
            # Ïã†Ìò∏ Î∂ÑÏÑù Îç∞Ïù¥ÌÑ∞ ÌÜµÍ≥Ñ
            signal_query = """
                SELECT 
                    COUNT(*) as total_signals,
                    COUNT(CASE WHEN signal_passed = 1 THEN 1 END) as passed_signals,
                    COUNT(CASE WHEN signal_passed = 0 THEN 1 END) as failed_signals,
                    AVG(signal_strength) as avg_strength,
                    COUNT(DISTINCT stock_code) as unique_stocks,
                    COUNT(DISTINCT strategy_type) as unique_strategies,
                    MIN(timestamp) as earliest_signal,
                    MAX(timestamp) as latest_signal
                FROM signal_analysis
            """
            signal_stats = pd.read_sql_query(signal_query, conn).iloc[0].to_dict()
            stats['signals'] = signal_stats
            
            # Îß§Ïàò ÏãúÎèÑ Îç∞Ïù¥ÌÑ∞ ÌÜµÍ≥Ñ
            buy_query = """
                SELECT 
                    COUNT(*) as total_attempts,
                    COUNT(CASE WHEN attempt_result = 'SUCCESS' THEN 1 END) as successful_buys,
                    COUNT(CASE WHEN attempt_result = 'FAILED' THEN 1 END) as failed_buys,
                    AVG(signal_strength) as avg_buy_strength,
                    SUM(total_amount) as total_buy_amount,
                    AVG(total_amount) as avg_buy_amount
                FROM buy_attempts
            """
            buy_stats = pd.read_sql_query(buy_query, conn).iloc[0].to_dict()
            stats['buy_attempts'] = buy_stats
            
            # ÏÑ±Í≥µÎ•† Í≥ÑÏÇ∞
            if signal_stats['total_signals'] > 0:
                stats['signal_pass_rate'] = signal_stats['passed_signals'] / signal_stats['total_signals']
            else:
                stats['signal_pass_rate'] = 0
                
            if buy_stats['total_attempts'] > 0:
                stats['buy_success_rate'] = buy_stats['successful_buys'] / buy_stats['total_attempts']
            else:
                stats['buy_success_rate'] = 0
        
        return stats

    def analyze_signal_failures(self) -> pd.DataFrame:
        """üö´ Ïã†Ìò∏ Ïã§Ìå® ÏõêÏù∏ Î∂ÑÏÑù"""
        with sqlite3.connect(str(self.db_path)) as conn:
            query = """
                SELECT 
                    signal_reason,
                    strategy_type,
                    COUNT(*) as failure_count,
                    AVG(signal_strength) as avg_strength,
                    AVG(signal_threshold) as avg_threshold,
                    AVG(signal_strength - signal_threshold) as avg_strength_gap
                FROM signal_analysis 
                WHERE signal_passed = 0
                GROUP BY signal_reason, strategy_type
                ORDER BY failure_count DESC
            """
            return pd.read_sql_query(query, conn)

    def analyze_strength_distribution(self) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """üìà Ïã†Ìò∏ Í∞ïÎèÑ Î∂ÑÌè¨ Î∂ÑÏÑù"""
        with sqlite3.connect(str(self.db_path)) as conn:
            # Ï†ÑÏ≤¥ Ïã†Ìò∏ Í∞ïÎèÑ Î∂ÑÌè¨
            overall_query = """
                SELECT 
                    ROUND(signal_strength, 1) as strength_range,
                    COUNT(*) as count,
                    AVG(CASE WHEN signal_passed = 1 THEN 1.0 ELSE 0.0 END) as pass_rate
                FROM signal_analysis
                GROUP BY ROUND(signal_strength, 1)
                ORDER BY strength_range
            """
            overall_dist = pd.read_sql_query(overall_query, conn)
            
            # Ï†ÑÎûµÎ≥Ñ Ïã†Ìò∏ Í∞ïÎèÑ Î∂ÑÌè¨
            strategy_query = """
                SELECT 
                    strategy_type,
                    ROUND(signal_strength, 1) as strength_range,
                    COUNT(*) as count,
                    AVG(CASE WHEN signal_passed = 1 THEN 1.0 ELSE 0.0 END) as pass_rate
                FROM signal_analysis
                GROUP BY strategy_type, ROUND(signal_strength, 1)
                ORDER BY strategy_type, strength_range
            """
            strategy_dist = pd.read_sql_query(strategy_query, conn)
            
        return overall_dist, strategy_dist

    def analyze_disparity_patterns(self) -> pd.DataFrame:
        """üìä Ïù¥Í≤©ÎèÑ Ìå®ÌÑ¥ Î∂ÑÏÑù"""
        with sqlite3.connect(str(self.db_path)) as conn:
            query = """
                SELECT 
                    CASE 
                        WHEN disparity_20d <= 90 THEN 'Í≥ºÎß§ÎèÑ (‚â§90%)'
                        WHEN disparity_20d <= 110 THEN 'Ï§ëÎ¶Ω (90-110%)'
                        WHEN disparity_20d <= 125 THEN 'ÏïΩÍ∞ÑÍ≥ºÎß§Ïàò (110-125%)'
                        ELSE 'Í≥ºÎß§Ïàò (>125%)'
                    END as disparity_range,
                    strategy_type,
                    COUNT(*) as signal_count,
                    AVG(CASE WHEN signal_passed = 1 THEN 1.0 ELSE 0.0 END) as pass_rate,
                    AVG(signal_strength) as avg_strength
                FROM signal_analysis 
                WHERE disparity_20d IS NOT NULL
                GROUP BY 
                    CASE 
                        WHEN disparity_20d <= 90 THEN 'Í≥ºÎß§ÎèÑ (‚â§90%)'
                        WHEN disparity_20d <= 110 THEN 'Ï§ëÎ¶Ω (90-110%)'
                        WHEN disparity_20d <= 125 THEN 'ÏïΩÍ∞ÑÍ≥ºÎß§Ïàò (110-125%)'
                        ELSE 'Í≥ºÎß§Ïàò (>125%)'
                    END,
                    strategy_type
                ORDER BY pass_rate DESC
            """
            return pd.read_sql_query(query, conn)

    def analyze_time_patterns(self) -> pd.DataFrame:
        """‚è∞ ÏãúÍ∞ÑÎåÄÎ≥Ñ Ìå®ÌÑ¥ Î∂ÑÏÑù"""
        with sqlite3.connect(str(self.db_path)) as conn:
            query = """
                SELECT 
                    strftime('%H', timestamp) as hour,
                    COUNT(*) as signal_count,
                    AVG(CASE WHEN signal_passed = 1 THEN 1.0 ELSE 0.0 END) as pass_rate,
                    AVG(signal_strength) as avg_strength
                FROM signal_analysis
                GROUP BY strftime('%H', timestamp)
                ORDER BY hour
            """
            return pd.read_sql_query(query, conn)

    def generate_feature_dataset(self, lookback_hours: int = 24) -> pd.DataFrame:
        """ü§ñ Î®∏Ïã†Îü¨ÎãùÏö© ÌîºÏ≤ò Îç∞Ïù¥ÌÑ∞ÏÖã ÏÉùÏÑ±"""
        with sqlite3.connect(str(self.db_path)) as conn:
            query = """
                SELECT 
                    stock_code,
                    timestamp,
                    strategy_type,
                    signal_strength,
                    signal_threshold,
                    signal_passed,
                    current_price,
                    volume,
                    volume_ratio,
                    rsi,
                    macd,
                    bb_position,
                    disparity_5d,
                    disparity_20d,
                    disparity_60d,
                    price_change_pct,
                    signal_reason
                FROM signal_analysis
                ORDER BY timestamp
            """
            df = pd.read_sql_query(query, conn)
            
            if df.empty:
                return df
                
            # ÏãúÍ∞Ñ Î≥ÄÌôò
            df['timestamp'] = pd.to_datetime(df['timestamp'])
            
            # Ïπ¥ÌÖåÍ≥†Î¶¨ Î≥ÄÏàò Ïù∏ÏΩîÎî©
            df['strategy_encoded'] = pd.Categorical(df['strategy_type']).codes
            
            # ÌååÏÉù ÌîºÏ≤ò ÏÉùÏÑ±
            df['strength_threshold_ratio'] = df['signal_strength'] / (df['signal_threshold'] + 0.001)
            df['disparity_spread'] = df['disparity_5d'] - df['disparity_20d']
            df['hour'] = df['timestamp'].dt.hour
            df['is_morning'] = (df['hour'] >= 9) & (df['hour'] <= 11)
            df['is_afternoon'] = (df['hour'] >= 13) & (df['hour'] <= 15)
            
            return df

    def create_visualizations(self, save_dir: str = "analysis/plots"):
        """üìä ÏãúÍ∞ÅÌôî ÏÉùÏÑ±"""
        save_path = Path(save_dir)
        save_path.mkdir(exist_ok=True, parents=True)
        
        # 1. Í∏∞Î≥∏ ÌÜµÍ≥Ñ
        stats = self.get_basic_stats()
        self._plot_basic_stats(stats, save_path)
        
        # 2. Ïã†Ìò∏ Ïã§Ìå® ÏõêÏù∏
        failure_analysis = self.analyze_signal_failures()
        if not failure_analysis.empty:
            self._plot_failure_reasons(failure_analysis, save_path)
        
        # 3. Ïã†Ìò∏ Í∞ïÎèÑ Î∂ÑÌè¨
        overall_dist, strategy_dist = self.analyze_strength_distribution()
        if not overall_dist.empty:
            self._plot_strength_distribution(overall_dist, strategy_dist, save_path)
        
        # 4. Ïù¥Í≤©ÎèÑ Ìå®ÌÑ¥
        disparity_patterns = self.analyze_disparity_patterns()
        if not disparity_patterns.empty:
            self._plot_disparity_patterns(disparity_patterns, save_path)
        
        # 5. ÏãúÍ∞ÑÎåÄ Ìå®ÌÑ¥
        time_patterns = self.analyze_time_patterns()
        if not time_patterns.empty:
            self._plot_time_patterns(time_patterns, save_path)
        
        print(f"üìä ÏãúÍ∞ÅÌôî Ï†ÄÏû• ÏôÑÎ£å: {save_path}")

    def _plot_basic_stats(self, stats: Dict, save_path: Path):
        """Í∏∞Î≥∏ ÌÜµÍ≥Ñ ÏãúÍ∞ÅÌôî"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('üéØ Í∏∞Î≥∏ ÌÜµÍ≥Ñ ÌòÑÌô©', fontsize=16, fontweight='bold')
        
        # Ïã†Ìò∏ ÏÑ±Í≥µ/Ïã§Ìå® ÎπÑÏú®
        signal_data = [stats['signals']['passed_signals'], stats['signals']['failed_signals']]
        signal_labels = ['ÏÑ±Í≥µ', 'Ïã§Ìå®']
        axes[0, 0].pie(signal_data, labels=signal_labels, autopct='%1.1f%%', startangle=90)
        axes[0, 0].set_title(f"Ïã†Ìò∏ ÌÜµÍ≥ºÏú® ({stats['signal_pass_rate']:.1%})")
        
        # Îß§Ïàò ÏÑ±Í≥µ/Ïã§Ìå® ÎπÑÏú®
        buy_data = [stats['buy_attempts']['successful_buys'], stats['buy_attempts']['failed_buys']]
        buy_labels = ['ÏÑ±Í≥µ', 'Ïã§Ìå®']
        if sum(buy_data) > 0:
            axes[0, 1].pie(buy_data, labels=buy_labels, autopct='%1.1f%%', startangle=90)
            axes[0, 1].set_title(f"Îß§Ïàò ÏÑ±Í≥µÎ•† ({stats['buy_success_rate']:.1%})")
        else:
            axes[0, 1].text(0.5, 0.5, 'Îß§Ïàò ÏãúÎèÑ ÏóÜÏùå', ha='center', va='center')
            axes[0, 1].set_title('Îß§Ïàò ÏÑ±Í≥µÎ•†')
        
        # Ïã†Ìò∏ Í∞ïÎèÑ ÌûàÏä§ÌÜ†Í∑∏Îû® (Í∞ÑÎã®Ìïú Î≤ÑÏ†Ñ)
        axes[1, 0].bar(['ÌèâÍ∑† Ïã†Ìò∏ Í∞ïÎèÑ'], [stats['signals']['avg_strength']], color='skyblue')
        axes[1, 0].set_title('ÌèâÍ∑† Ïã†Ìò∏ Í∞ïÎèÑ')
        axes[1, 0].set_ylim(0, 1)
        
        # Ï¢ÖÎ™© Î∞è Ï†ÑÎûµ Ïàò
        categories = ['Í≥†Ïú† Ï¢ÖÎ™© Ïàò', 'Í≥†Ïú† Ï†ÑÎûµ Ïàò']
        counts = [stats['signals']['unique_stocks'], stats['signals']['unique_strategies']]
        axes[1, 1].bar(categories, counts, color=['orange', 'green'])
        axes[1, 1].set_title('Îç∞Ïù¥ÌÑ∞ Îã§ÏñëÏÑ±')
        
        plt.tight_layout()
        plt.savefig(save_path / 'basic_stats.png', dpi=300, bbox_inches='tight')
        plt.close()

    def _plot_failure_reasons(self, failure_df: pd.DataFrame, save_path: Path):
        """Ïã§Ìå® ÏõêÏù∏ ÏãúÍ∞ÅÌôî"""
        fig, axes = plt.subplots(1, 2, figsize=(20, 8))
        
        # Ïã§Ìå® ÏõêÏù∏ ÏàúÏúÑ (ÏÉÅÏúÑ 10Í∞ú)
        top_failures = failure_df.groupby('signal_reason')['failure_count'].sum().nlargest(10)
        axes[0].barh(range(len(top_failures)), top_failures.values)
        axes[0].set_yticks(range(len(top_failures)))
        axes[0].set_yticklabels([reason[:50] + '...' if len(reason) > 50 else reason 
                                for reason in top_failures.index])
        axes[0].set_xlabel('Ïã§Ìå® ÌöüÏàò')
        axes[0].set_title('üö´ Ïã†Ìò∏ Ïã§Ìå® ÏõêÏù∏ Top 10')
        
        # Ï†ÑÎûµÎ≥Ñ Ïã§Ìå® Î∂ÑÌè¨
        strategy_failures = failure_df.groupby('strategy_type')['failure_count'].sum()
        axes[1].pie(strategy_failures.values, labels=strategy_failures.index, autopct='%1.1f%%')
        axes[1].set_title('üìà Ï†ÑÎûµÎ≥Ñ Ïã§Ìå® Î∂ÑÌè¨')
        
        plt.tight_layout()
        plt.savefig(save_path / 'failure_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()

    def _plot_strength_distribution(self, overall_df: pd.DataFrame, strategy_df: pd.DataFrame, save_path: Path):
        """Ïã†Ìò∏ Í∞ïÎèÑ Î∂ÑÌè¨ ÏãúÍ∞ÅÌôî"""
        fig, axes = plt.subplots(2, 1, figsize=(15, 12))
        
        # Ï†ÑÏ≤¥ Ïã†Ìò∏ Í∞ïÎèÑ Î∂ÑÌè¨ÏôÄ ÌÜµÍ≥ºÏú®
        ax1 = axes[0]
        ax2 = ax1.twinx()
        
        bars = ax1.bar(overall_df['strength_range'], overall_df['count'], alpha=0.7, color='skyblue')
        line = ax2.plot(overall_df['strength_range'], overall_df['pass_rate'] * 100, 
                       color='red', marker='o', linewidth=2)
        
        ax1.set_xlabel('Ïã†Ìò∏ Í∞ïÎèÑ')
        ax1.set_ylabel('Ïã†Ìò∏ Í∞úÏàò', color='blue')
        ax2.set_ylabel('ÌÜµÍ≥ºÏú® (%)', color='red')
        ax1.set_title('üìä Ïã†Ìò∏ Í∞ïÎèÑÎ≥Ñ Î∂ÑÌè¨ Î∞è ÌÜµÍ≥ºÏú®')
        
        # Ï†ÑÎûµÎ≥Ñ Ïã†Ìò∏ Í∞ïÎèÑ Î∂ÑÌè¨ (ÌûàÌä∏Îßµ)
        pivot_data = strategy_df.pivot(index='strategy_type', columns='strength_range', values='count').fillna(0)
        sns.heatmap(pivot_data, annot=True, fmt='.0f', cmap='YlOrRd', ax=axes[1])
        axes[1].set_title('üéØ Ï†ÑÎûµÎ≥Ñ Ïã†Ìò∏ Í∞ïÎèÑ Î∂ÑÌè¨')
        axes[1].set_xlabel('Ïã†Ìò∏ Í∞ïÎèÑ')
        axes[1].set_ylabel('Ï†ÑÎûµ')
        
        plt.tight_layout()
        plt.savefig(save_path / 'strength_distribution.png', dpi=300, bbox_inches='tight')
        plt.close()

    def _plot_disparity_patterns(self, disparity_df: pd.DataFrame, save_path: Path):
        """Ïù¥Í≤©ÎèÑ Ìå®ÌÑ¥ ÏãúÍ∞ÅÌôî"""
        fig, axes = plt.subplots(1, 2, figsize=(20, 8))
        
        # Ïù¥Í≤©ÎèÑ Íµ¨Í∞ÑÎ≥Ñ ÌÜµÍ≥ºÏú®
        overall_disparity = disparity_df.groupby('disparity_range').agg({
            'signal_count': 'sum',
            'pass_rate': 'mean'
        }).reset_index()
        
        ax1 = axes[0]
        ax2 = ax1.twinx()
        
        bars = ax1.bar(overall_disparity['disparity_range'], overall_disparity['signal_count'], 
                      alpha=0.7, color='lightgreen')
        line = ax2.plot(overall_disparity['disparity_range'], overall_disparity['pass_rate'] * 100, 
                       color='red', marker='o', linewidth=2)
        
        ax1.set_ylabel('Ïã†Ìò∏ Í∞úÏàò', color='green')
        ax2.set_ylabel('ÌÜµÍ≥ºÏú® (%)', color='red')
        ax1.set_title('üìà Ïù¥Í≤©ÎèÑ Íµ¨Í∞ÑÎ≥Ñ Ïã†Ìò∏ Î∂ÑÌè¨ Î∞è ÌÜµÍ≥ºÏú®')
        plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45)
        
        # Ï†ÑÎûµÎ≥Ñ Ïù¥Í≤©ÎèÑ Ìå®ÌÑ¥ ÌûàÌä∏Îßµ
        pivot_data = disparity_df.pivot(index='strategy_type', columns='disparity_range', values='pass_rate')
        sns.heatmap(pivot_data, annot=True, fmt='.2f', cmap='RdYlGn', ax=axes[1], 
                   vmin=0, vmax=1, cbar_kws={'label': 'ÌÜµÍ≥ºÏú®'})
        axes[1].set_title('üéØ Ï†ÑÎûµÎ≥Ñ Ïù¥Í≤©ÎèÑ Íµ¨Í∞Ñ ÌÜµÍ≥ºÏú®')
        axes[1].set_xlabel('Ïù¥Í≤©ÎèÑ Íµ¨Í∞Ñ')
        axes[1].set_ylabel('Ï†ÑÎûµ')
        
        plt.tight_layout()
        plt.savefig(save_path / 'disparity_patterns.png', dpi=300, bbox_inches='tight')
        plt.close()

    def _plot_time_patterns(self, time_df: pd.DataFrame, save_path: Path):
        """ÏãúÍ∞ÑÎåÄ Ìå®ÌÑ¥ ÏãúÍ∞ÅÌôî"""
        fig, axes = plt.subplots(2, 1, figsize=(15, 10))
        
        # ÏãúÍ∞ÑÎåÄÎ≥Ñ Ïã†Ìò∏ Í∞úÏàò
        axes[0].bar(time_df['hour'], time_df['signal_count'], color='lightblue', alpha=0.7)
        axes[0].set_xlabel('ÏãúÍ∞Ñ (24ÏãúÍ∞Ñ)')
        axes[0].set_ylabel('Ïã†Ìò∏ Í∞úÏàò')
        axes[0].set_title('‚è∞ ÏãúÍ∞ÑÎåÄÎ≥Ñ Ïã†Ìò∏ Î∞úÏÉù ÎπàÎèÑ')
        axes[0].set_xticks(range(0, 24, 2))
        
        # ÏãúÍ∞ÑÎåÄÎ≥Ñ ÌÜµÍ≥ºÏú®Í≥º ÌèâÍ∑† Í∞ïÎèÑ
        ax1 = axes[1]
        ax2 = ax1.twinx()
        
        line1 = ax1.plot(time_df['hour'], time_df['pass_rate'] * 100, 
                        color='red', marker='o', linewidth=2, label='ÌÜµÍ≥ºÏú®')
        line2 = ax2.plot(time_df['hour'], time_df['avg_strength'], 
                        color='blue', marker='s', linewidth=2, label='ÌèâÍ∑† Í∞ïÎèÑ')
        
        ax1.set_xlabel('ÏãúÍ∞Ñ (24ÏãúÍ∞Ñ)')
        ax1.set_ylabel('ÌÜµÍ≥ºÏú® (%)', color='red')
        ax2.set_ylabel('ÌèâÍ∑† Ïã†Ìò∏ Í∞ïÎèÑ', color='blue')
        ax1.set_title('üìä ÏãúÍ∞ÑÎåÄÎ≥Ñ Ïã†Ìò∏ ÌíàÏßà')
        ax1.set_xticks(range(0, 24, 2))
        
        # Î≤îÎ°Ä
        lines1, labels1 = ax1.get_legend_handles_labels()
        lines2, labels2 = ax2.get_legend_handles_labels()
        ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper right')
        
        plt.tight_layout()
        plt.savefig(save_path / 'time_patterns.png', dpi=300, bbox_inches='tight')
        plt.close()

    def export_ml_dataset(self, output_path: str = "analysis/ml_dataset.csv"):
        """ü§ñ Î®∏Ïã†Îü¨ÎãùÏö© Îç∞Ïù¥ÌÑ∞ÏÖã ÎÇ¥Î≥¥ÎÇ¥Í∏∞"""
        df = self.generate_feature_dataset()
        
        if df.empty:
            print("‚ö†Ô∏è ÎÇ¥Î≥¥ÎÇº Îç∞Ïù¥ÌÑ∞Í∞Ä ÏóÜÏäµÎãàÎã§.")
            return
        
        # Í≤∞Ï∏°Í∞í Ï≤òÎ¶¨
        numeric_columns = df.select_dtypes(include=[np.number]).columns
        df[numeric_columns] = df[numeric_columns].fillna(df[numeric_columns].median())
        
        # ÌååÏùº Ï†ÄÏû•
        output_file = Path(output_path)
        output_file.parent.mkdir(exist_ok=True, parents=True)
        df.to_csv(output_file, index=False, encoding='utf-8-sig')
        
        print(f"üìÅ ML Îç∞Ïù¥ÌÑ∞ÏÖã Ï†ÄÏû• ÏôÑÎ£å: {output_file}")
        print(f"üìä Îç∞Ïù¥ÌÑ∞ ÌÅ¨Í∏∞: {df.shape}")
        print(f"üìà Target Î∂ÑÌè¨: {df['signal_passed'].value_counts().to_dict()}")

    def print_summary_report(self):
        """üìã ÏöîÏïΩ Î≥¥Í≥†ÏÑú Ï∂úÎ†•"""
        print("=" * 80)
        print("üéØ Î®∏Ïã†Îü¨Îãù Îç∞Ïù¥ÌÑ∞ Î∂ÑÏÑù Î≥¥Í≥†ÏÑú")
        print("=" * 80)
        
        # Í∏∞Î≥∏ ÌÜµÍ≥Ñ
        stats = self.get_basic_stats()
        print(f"\nüìä Í∏∞Î≥∏ ÌÜµÍ≥Ñ:")
        print(f"  ‚Ä¢ Ï¥ù Ïã†Ìò∏ Ïàò: {stats['signals']['total_signals']:,}Í∞ú")
        print(f"  ‚Ä¢ Ïã†Ìò∏ ÌÜµÍ≥ºÏú®: {stats['signal_pass_rate']:.1%}")
        print(f"  ‚Ä¢ Îß§Ïàò ÏÑ±Í≥µÎ•†: {stats['buy_success_rate']:.1%}")
        print(f"  ‚Ä¢ Í≥†Ïú† Ï¢ÖÎ™© Ïàò: {stats['signals']['unique_stocks']:,}Í∞ú")
        print(f"  ‚Ä¢ Í≥†Ïú† Ï†ÑÎûµ Ïàò: {stats['signals']['unique_strategies']:,}Í∞ú")
        
        # Ï£ºÏöî Ïã§Ìå® ÏõêÏù∏
        failure_analysis = self.analyze_signal_failures()
        if not failure_analysis.empty:
            print(f"\nüö´ Ï£ºÏöî Ïã§Ìå® ÏõêÏù∏ Top 5:")
            top_failures = failure_analysis.groupby('signal_reason')['failure_count'].sum().nlargest(5)
            for i, (reason, count) in enumerate(top_failures.items(), 1):
                print(f"  {i}. {reason}: {count:,}Ìöå")
        
        # Ïù¥Í≤©ÎèÑ Î∂ÑÏÑù
        disparity_patterns = self.analyze_disparity_patterns()
        if not disparity_patterns.empty:
            print(f"\nüìà Ïù¥Í≤©ÎèÑ Íµ¨Í∞ÑÎ≥Ñ ÌÜµÍ≥ºÏú®:")
            overall_disparity = disparity_patterns.groupby('disparity_range').agg({
                'signal_count': 'sum',
                'pass_rate': 'mean'
            }).round(3)
            for range_name, row in overall_disparity.iterrows():
                print(f"  ‚Ä¢ {range_name}: {row['pass_rate']:.1%} ({row['signal_count']:,}Í∞ú)")
        
        print("\n" + "=" * 80)

    def analyze_enhanced_patterns(self) -> pd.DataFrame:
        """üÜï ÌôïÏû•Îêú Ìå®ÌÑ¥ Î∂ÑÏÑù"""
        with sqlite3.connect(str(self.db_path)) as conn:
            query = """
                SELECT 
                    strategy_type,
                    CASE 
                        WHEN hour_of_day BETWEEN 9 AND 11 THEN 'Ïò§Ï†Ñ'
                        WHEN hour_of_day BETWEEN 12 AND 14 THEN 'Ïò§ÌõÑ' 
                        ELSE 'Í∏∞ÌÉÄ'
                    END as time_period,
                    CASE 
                        WHEN disparity_20d <= 90 THEN 'Í≥ºÎß§ÎèÑ'
                        WHEN disparity_20d <= 110 THEN 'Ï§ëÎ¶Ω'
                        ELSE 'Í≥ºÎß§Ïàò'
                    END as disparity_zone,
                    CASE 
                        WHEN volume_ratio_20d >= 2.0 THEN 'Í≥†Í±∞ÎûòÎüâ'
                        WHEN volume_ratio_20d >= 1.5 THEN 'Ï§ëÍ±∞ÎûòÎüâ'
                        ELSE 'Ï†ÄÍ±∞ÎûòÎüâ'
                    END as volume_zone,
                    COUNT(*) as signal_count,
                    AVG(CASE WHEN signal_passed = 1 THEN 1.0 ELSE 0.0 END) as pass_rate,
                    AVG(signal_strength) as avg_strength,
                    AVG(rsi) as avg_rsi,
                    AVG(volatility_20d) as avg_volatility
                FROM signal_analysis 
                WHERE signal_strength IS NOT NULL
                GROUP BY strategy_type, time_period, disparity_zone, volume_zone
                HAVING signal_count >= 5
                ORDER BY pass_rate DESC, signal_count DESC
            """
            return pd.read_sql_query(query, conn)

    def analyze_market_conditions_impact(self) -> pd.DataFrame:
        """üÜï ÏãúÏû• ÏÉÅÌô©Î≥Ñ ÏÑ±Í≥º Î∂ÑÏÑù"""
        with sqlite3.connect(str(self.db_path)) as conn:
            query = """
                SELECT 
                    CASE 
                        WHEN market_volatility <= 0.1 THEN 'ÏïàÏ†ï'
                        WHEN market_volatility <= 0.2 THEN 'Î≥¥ÌÜµ'
                        ELSE 'Í≥†Î≥ÄÎèô'
                    END as volatility_regime,
                    CASE 
                        WHEN kospi_change_pct >= 1.0 THEN 'Í∞ïÏÑ∏Ïû•'
                        WHEN kospi_change_pct >= -1.0 THEN 'Î≥¥Ìï©Ïû•'
                        ELSE 'ÏïΩÏÑ∏Ïû•'
                    END as market_trend,
                    strategy_type,
                    COUNT(*) as attempt_count,
                    AVG(CASE WHEN attempt_result = 'SUCCESS' THEN 1.0 ELSE 0.0 END) as success_rate,
                    AVG(signal_strength) as avg_signal_strength
                FROM buy_attempts ba
                WHERE kospi_change_pct IS NOT NULL AND market_volatility IS NOT NULL
                GROUP BY volatility_regime, market_trend, strategy_type
                HAVING attempt_count >= 3
                ORDER BY success_rate DESC
            """
            return pd.read_sql_query(query, conn)

    def get_feature_importance_data(self) -> pd.DataFrame:
        """üÜï ÌîºÏ≤ò Ï§ëÏöîÎèÑ Î∂ÑÏÑùÏö© Îç∞Ïù¥ÌÑ∞"""
        with sqlite3.connect(str(self.db_path)) as conn:
            query = """
                SELECT 
                    signal_strength, disparity_20d, volume_ratio_20d, rsi, macd,
                    bb_position, volatility_20d, momentum_20d, hour_of_day,
                    CASE WHEN day_of_week IN (0,1,2,3,4) THEN 1 ELSE 0 END as is_weekday,
                    CASE WHEN is_opening_hour = 1 THEN 1 ELSE 0 END as is_opening,
                    CASE WHEN is_closing_hour = 1 THEN 1 ELSE 0 END as is_closing,
                    signal_passed as target
                FROM signal_analysis 
                WHERE signal_strength IS NOT NULL 
                AND disparity_20d IS NOT NULL 
                AND volume_ratio_20d IS NOT NULL
            """
            return pd.read_sql_query(query, conn)


if __name__ == "__main__":
    # ÏÇ¨Ïö© ÏòàÏãú
    try:
        analyzer = MLDataAnalyzer()
        
        # ÏöîÏïΩ Î≥¥Í≥†ÏÑú Ï∂úÎ†•
        analyzer.print_summary_report()
        
        # ÏãúÍ∞ÅÌôî ÏÉùÏÑ±
        analyzer.create_visualizations()
        
        # Î®∏Ïã†Îü¨Îãù Îç∞Ïù¥ÌÑ∞ÏÖã ÎÇ¥Î≥¥ÎÇ¥Í∏∞
        analyzer.export_ml_dataset()
        
        print("\n‚úÖ Î∂ÑÏÑù ÏôÑÎ£å!")
        
    except FileNotFoundError as e:
        print(f"‚ùå Ïò§Î•ò: {e}")
        print("üí° ÌûåÌä∏: ÏãúÏä§ÌÖúÏùÑ Ïã§ÌñâÌïòÏó¨ Îç∞Ïù¥ÌÑ∞Î•º ÏàòÏßëÌïú ÌõÑ Îã§Ïãú ÏãúÎèÑÌï¥Ï£ºÏÑ∏Ïöî.")
    except Exception as e:
        print(f"‚ùå ÏòàÏÉÅÏπò Î™ªÌïú Ïò§Î•ò: {e}") 