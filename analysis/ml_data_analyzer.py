"""
ë¨¸ì‹ ëŸ¬ë‹ ë°ì´í„° ë¶„ì„ ë„êµ¬
ìˆ˜ì§‘ëœ ì‹ í˜¸/ë§¤ìˆ˜ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ íŒ¨í„´ê³¼ ì¸ì‚¬ì´íŠ¸ ë„ì¶œ
"""
import sqlite3
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
import json
from pathlib import Path

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'Malgun Gothic'
plt.rcParams['axes.unicode_minus'] = False

class MLDataAnalyzer:
    """ğŸ’¡ ë¨¸ì‹ ëŸ¬ë‹ ë°ì´í„° ë¶„ì„ê¸°"""

    def __init__(self, db_path: str = "data/ml_training_data.db"):
        """ì´ˆê¸°í™”"""
        self.db_path = Path(db_path)
        if not self.db_path.exists():
            raise FileNotFoundError(f"ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.db_path}")
        
        print(f"ğŸ” ML ë°ì´í„° ë¶„ì„ê¸° ì´ˆê¸°í™”: {self.db_path}")

    def get_basic_stats(self) -> Dict:
        """ğŸ“Š ê¸°ë³¸ í†µê³„ ì •ë³´"""
        with sqlite3.connect(str(self.db_path)) as conn:
            stats = {}
            
            # ì‹ í˜¸ ë¶„ì„ ë°ì´í„° í†µê³„
            signal_query = """
                SELECT 
                    COUNT(*) as total_signals,
                    COUNT(CASE WHEN signal_passed = 1 THEN 1 END) as passed_signals,
                    COUNT(CASE WHEN signal_passed = 0 THEN 1 END) as failed_signals,
                    AVG(signal_strength) as avg_strength,
                    COUNT(DISTINCT stock_code) as unique_stocks,
                    COUNT(DISTINCT strategy_type) as unique_strategies,
                    MIN(timestamp) as earliest_signal,
                    MAX(timestamp) as latest_signal
                FROM signal_analysis
            """
            signal_stats = pd.read_sql_query(signal_query, conn).iloc[0].to_dict()
            stats['signals'] = signal_stats
            
            # ë§¤ìˆ˜ ì‹œë„ ë°ì´í„° í†µê³„
            buy_query = """
                SELECT 
                    COUNT(*) as total_attempts,
                    COUNT(CASE WHEN attempt_result = 'SUCCESS' THEN 1 END) as successful_buys,
                    COUNT(CASE WHEN attempt_result = 'FAILED' THEN 1 END) as failed_buys,
                    AVG(signal_strength) as avg_buy_strength,
                    SUM(total_amount) as total_buy_amount,
                    AVG(total_amount) as avg_buy_amount
                FROM buy_attempts
            """
            buy_stats = pd.read_sql_query(buy_query, conn).iloc[0].to_dict()
            stats['buy_attempts'] = buy_stats
            
            # ì„±ê³µë¥  ê³„ì‚°
            if signal_stats['total_signals'] > 0:
                stats['signal_pass_rate'] = signal_stats['passed_signals'] / signal_stats['total_signals']
            else:
                stats['signal_pass_rate'] = 0
                
            if buy_stats['total_attempts'] > 0:
                stats['buy_success_rate'] = buy_stats['successful_buys'] / buy_stats['total_attempts']
            else:
                stats['buy_success_rate'] = 0
        
        return stats

    def analyze_signal_failures(self) -> pd.DataFrame:
        """ğŸš« ì‹ í˜¸ ì‹¤íŒ¨ ì›ì¸ ë¶„ì„"""
        with sqlite3.connect(str(self.db_path)) as conn:
            query = """
                SELECT 
                    signal_reason,
                    strategy_type,
                    COUNT(*) as failure_count,
                    AVG(signal_strength) as avg_strength,
                    AVG(signal_threshold) as avg_threshold,
                    AVG(signal_strength - signal_threshold) as avg_strength_gap
                FROM signal_analysis 
                WHERE signal_passed = 0
                GROUP BY signal_reason, strategy_type
                ORDER BY failure_count DESC
            """
            return pd.read_sql_query(query, conn)

    def analyze_strength_distribution(self) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """ğŸ“ˆ ì‹ í˜¸ ê°•ë„ ë¶„í¬ ë¶„ì„"""
        with sqlite3.connect(str(self.db_path)) as conn:
            # ì „ì²´ ì‹ í˜¸ ê°•ë„ ë¶„í¬
            overall_query = """
                SELECT 
                    ROUND(signal_strength, 1) as strength_range,
                    COUNT(*) as count,
                    AVG(CASE WHEN signal_passed = 1 THEN 1.0 ELSE 0.0 END) as pass_rate
                FROM signal_analysis
                GROUP BY ROUND(signal_strength, 1)
                ORDER BY strength_range
            """
            overall_dist = pd.read_sql_query(overall_query, conn)
            
            # ì „ëµë³„ ì‹ í˜¸ ê°•ë„ ë¶„í¬
            strategy_query = """
                SELECT 
                    strategy_type,
                    ROUND(signal_strength, 1) as strength_range,
                    COUNT(*) as count,
                    AVG(CASE WHEN signal_passed = 1 THEN 1.0 ELSE 0.0 END) as pass_rate
                FROM signal_analysis
                GROUP BY strategy_type, ROUND(signal_strength, 1)
                ORDER BY strategy_type, strength_range
            """
            strategy_dist = pd.read_sql_query(strategy_query, conn)
            
        return overall_dist, strategy_dist

    def analyze_disparity_patterns(self) -> pd.DataFrame:
        """ğŸ“Š ì´ê²©ë„ íŒ¨í„´ ë¶„ì„"""
        with sqlite3.connect(str(self.db_path)) as conn:
            query = """
                SELECT 
                    CASE 
                        WHEN disparity_20d <= 90 THEN 'ê³¼ë§¤ë„ (â‰¤90%)'
                        WHEN disparity_20d <= 110 THEN 'ì¤‘ë¦½ (90-110%)'
                        WHEN disparity_20d <= 125 THEN 'ì•½ê°„ê³¼ë§¤ìˆ˜ (110-125%)'
                        ELSE 'ê³¼ë§¤ìˆ˜ (>125%)'
                    END as disparity_range,
                    strategy_type,
                    COUNT(*) as signal_count,
                    AVG(CASE WHEN signal_passed = 1 THEN 1.0 ELSE 0.0 END) as pass_rate,
                    AVG(signal_strength) as avg_strength
                FROM signal_analysis 
                WHERE disparity_20d IS NOT NULL
                GROUP BY 
                    CASE 
                        WHEN disparity_20d <= 90 THEN 'ê³¼ë§¤ë„ (â‰¤90%)'
                        WHEN disparity_20d <= 110 THEN 'ì¤‘ë¦½ (90-110%)'
                        WHEN disparity_20d <= 125 THEN 'ì•½ê°„ê³¼ë§¤ìˆ˜ (110-125%)'
                        ELSE 'ê³¼ë§¤ìˆ˜ (>125%)'
                    END,
                    strategy_type
                ORDER BY pass_rate DESC
            """
            return pd.read_sql_query(query, conn)

    def analyze_time_patterns(self) -> pd.DataFrame:
        """â° ì‹œê°„ëŒ€ë³„ íŒ¨í„´ ë¶„ì„"""
        with sqlite3.connect(str(self.db_path)) as conn:
            query = """
                SELECT 
                    strftime('%H', timestamp) as hour,
                    COUNT(*) as signal_count,
                    AVG(CASE WHEN signal_passed = 1 THEN 1.0 ELSE 0.0 END) as pass_rate,
                    AVG(signal_strength) as avg_strength
                FROM signal_analysis
                GROUP BY strftime('%H', timestamp)
                ORDER BY hour
            """
            return pd.read_sql_query(query, conn)

    def generate_feature_dataset(self, lookback_hours: int = 24) -> pd.DataFrame:
        """ğŸ¤– ë¨¸ì‹ ëŸ¬ë‹ìš© í”¼ì²˜ ë°ì´í„°ì…‹ ìƒì„±"""
        with sqlite3.connect(str(self.db_path)) as conn:
            query = """
                SELECT 
                    stock_code,
                    timestamp,
                    strategy_type,
                    signal_strength,
                    signal_threshold,
                    signal_passed,
                    current_price,
                    volume,
                    volume_ratio,
                    rsi,
                    macd,
                    bb_position,
                    disparity_5d,
                    disparity_20d,
                    disparity_60d,
                    price_change_pct,
                    signal_reason
                FROM signal_analysis
                ORDER BY timestamp
            """
            df = pd.read_sql_query(query, conn)
            
            if df.empty:
                return df
                
            # ì‹œê°„ ë³€í™˜
            df['timestamp'] = pd.to_datetime(df['timestamp'])
            
            # ì¹´í…Œê³ ë¦¬ ë³€ìˆ˜ ì¸ì½”ë”©
            df['strategy_encoded'] = pd.Categorical(df['strategy_type']).codes
            
            # íŒŒìƒ í”¼ì²˜ ìƒì„±
            df['strength_threshold_ratio'] = df['signal_strength'] / (df['signal_threshold'] + 0.001)
            df['disparity_spread'] = df['disparity_5d'] - df['disparity_20d']
            df['hour'] = df['timestamp'].dt.hour
            df['is_morning'] = (df['hour'] >= 9) & (df['hour'] <= 11)
            df['is_afternoon'] = (df['hour'] >= 13) & (df['hour'] <= 15)
            
            return df

    def create_visualizations(self, save_dir: str = "analysis/plots"):
        """ğŸ“Š ì‹œê°í™” ìƒì„±"""
        save_path = Path(save_dir)
        save_path.mkdir(exist_ok=True, parents=True)
        
        # 1. ê¸°ë³¸ í†µê³„
        stats = self.get_basic_stats()
        self._plot_basic_stats(stats, save_path)
        
        # 2. ì‹ í˜¸ ì‹¤íŒ¨ ì›ì¸
        failure_analysis = self.analyze_signal_failures()
        if not failure_analysis.empty:
            self._plot_failure_reasons(failure_analysis, save_path)
        
        # 3. ì‹ í˜¸ ê°•ë„ ë¶„í¬
        overall_dist, strategy_dist = self.analyze_strength_distribution()
        if not overall_dist.empty:
            self._plot_strength_distribution(overall_dist, strategy_dist, save_path)
        
        # 4. ì´ê²©ë„ íŒ¨í„´
        disparity_patterns = self.analyze_disparity_patterns()
        if not disparity_patterns.empty:
            self._plot_disparity_patterns(disparity_patterns, save_path)
        
        # 5. ì‹œê°„ëŒ€ íŒ¨í„´
        time_patterns = self.analyze_time_patterns()
        if not time_patterns.empty:
            self._plot_time_patterns(time_patterns, save_path)
        
        print(f"ğŸ“Š ì‹œê°í™” ì €ì¥ ì™„ë£Œ: {save_path}")

    def _plot_basic_stats(self, stats: Dict, save_path: Path):
        """ê¸°ë³¸ í†µê³„ ì‹œê°í™”"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('ğŸ¯ ê¸°ë³¸ í†µê³„ í˜„í™©', fontsize=16, fontweight='bold')
        
        # ì‹ í˜¸ ì„±ê³µ/ì‹¤íŒ¨ ë¹„ìœ¨
        signal_data = [stats['signals']['passed_signals'], stats['signals']['failed_signals']]
        signal_labels = ['ì„±ê³µ', 'ì‹¤íŒ¨']
        axes[0, 0].pie(signal_data, labels=signal_labels, autopct='%1.1f%%', startangle=90)
        axes[0, 0].set_title(f"ì‹ í˜¸ í†µê³¼ìœ¨ ({stats['signal_pass_rate']:.1%})")
        
        # ë§¤ìˆ˜ ì„±ê³µ/ì‹¤íŒ¨ ë¹„ìœ¨
        buy_data = [stats['buy_attempts']['successful_buys'], stats['buy_attempts']['failed_buys']]
        buy_labels = ['ì„±ê³µ', 'ì‹¤íŒ¨']
        if sum(buy_data) > 0:
            axes[0, 1].pie(buy_data, labels=buy_labels, autopct='%1.1f%%', startangle=90)
            axes[0, 1].set_title(f"ë§¤ìˆ˜ ì„±ê³µë¥  ({stats['buy_success_rate']:.1%})")
        else:
            axes[0, 1].text(0.5, 0.5, 'ë§¤ìˆ˜ ì‹œë„ ì—†ìŒ', ha='center', va='center')
            axes[0, 1].set_title('ë§¤ìˆ˜ ì„±ê³µë¥ ')
        
        # ì‹ í˜¸ ê°•ë„ íˆìŠ¤í† ê·¸ë¨ (ê°„ë‹¨í•œ ë²„ì „)
        axes[1, 0].bar(['í‰ê·  ì‹ í˜¸ ê°•ë„'], [stats['signals']['avg_strength']], color='skyblue')
        axes[1, 0].set_title('í‰ê·  ì‹ í˜¸ ê°•ë„')
        axes[1, 0].set_ylim(0, 1)
        
        # ì¢…ëª© ë° ì „ëµ ìˆ˜
        categories = ['ê³ ìœ  ì¢…ëª© ìˆ˜', 'ê³ ìœ  ì „ëµ ìˆ˜']
        counts = [stats['signals']['unique_stocks'], stats['signals']['unique_strategies']]
        axes[1, 1].bar(categories, counts, color=['orange', 'green'])
        axes[1, 1].set_title('ë°ì´í„° ë‹¤ì–‘ì„±')
        
        plt.tight_layout()
        plt.savefig(save_path / 'basic_stats.png', dpi=300, bbox_inches='tight')
        plt.close()

    def _plot_failure_reasons(self, failure_df: pd.DataFrame, save_path: Path):
        """ì‹¤íŒ¨ ì›ì¸ ì‹œê°í™”"""
        fig, axes = plt.subplots(1, 2, figsize=(20, 8))
        
        # ì‹¤íŒ¨ ì›ì¸ ìˆœìœ„ (ìƒìœ„ 10ê°œ)
        top_failures = failure_df.groupby('signal_reason')['failure_count'].sum().nlargest(10)
        axes[0].barh(range(len(top_failures)), top_failures.values)
        axes[0].set_yticks(range(len(top_failures)))
        axes[0].set_yticklabels([reason[:50] + '...' if len(reason) > 50 else reason 
                                for reason in top_failures.index])
        axes[0].set_xlabel('ì‹¤íŒ¨ íšŸìˆ˜')
        axes[0].set_title('ğŸš« ì‹ í˜¸ ì‹¤íŒ¨ ì›ì¸ Top 10')
        
        # ì „ëµë³„ ì‹¤íŒ¨ ë¶„í¬
        strategy_failures = failure_df.groupby('strategy_type')['failure_count'].sum()
        axes[1].pie(strategy_failures.values, labels=strategy_failures.index, autopct='%1.1f%%')
        axes[1].set_title('ğŸ“ˆ ì „ëµë³„ ì‹¤íŒ¨ ë¶„í¬')
        
        plt.tight_layout()
        plt.savefig(save_path / 'failure_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()

    def _plot_strength_distribution(self, overall_df: pd.DataFrame, strategy_df: pd.DataFrame, save_path: Path):
        """ì‹ í˜¸ ê°•ë„ ë¶„í¬ ì‹œê°í™”"""
        fig, axes = plt.subplots(2, 1, figsize=(15, 12))
        
        # ì „ì²´ ì‹ í˜¸ ê°•ë„ ë¶„í¬ì™€ í†µê³¼ìœ¨
        ax1 = axes[0]
        ax2 = ax1.twinx()
        
        bars = ax1.bar(overall_df['strength_range'], overall_df['count'], alpha=0.7, color='skyblue')
        line = ax2.plot(overall_df['strength_range'], overall_df['pass_rate'] * 100, 
                       color='red', marker='o', linewidth=2)
        
        ax1.set_xlabel('ì‹ í˜¸ ê°•ë„')
        ax1.set_ylabel('ì‹ í˜¸ ê°œìˆ˜', color='blue')
        ax2.set_ylabel('í†µê³¼ìœ¨ (%)', color='red')
        ax1.set_title('ğŸ“Š ì‹ í˜¸ ê°•ë„ë³„ ë¶„í¬ ë° í†µê³¼ìœ¨')
        
        # ì „ëµë³„ ì‹ í˜¸ ê°•ë„ ë¶„í¬ (íˆíŠ¸ë§µ)
        pivot_data = strategy_df.pivot(index='strategy_type', columns='strength_range', values='count').fillna(0)
        sns.heatmap(pivot_data, annot=True, fmt='.0f', cmap='YlOrRd', ax=axes[1])
        axes[1].set_title('ğŸ¯ ì „ëµë³„ ì‹ í˜¸ ê°•ë„ ë¶„í¬')
        axes[1].set_xlabel('ì‹ í˜¸ ê°•ë„')
        axes[1].set_ylabel('ì „ëµ')
        
        plt.tight_layout()
        plt.savefig(save_path / 'strength_distribution.png', dpi=300, bbox_inches='tight')
        plt.close()

    def _plot_disparity_patterns(self, disparity_df: pd.DataFrame, save_path: Path):
        """ì´ê²©ë„ íŒ¨í„´ ì‹œê°í™”"""
        fig, axes = plt.subplots(1, 2, figsize=(20, 8))
        
        # ì´ê²©ë„ êµ¬ê°„ë³„ í†µê³¼ìœ¨
        overall_disparity = disparity_df.groupby('disparity_range').agg({
            'signal_count': 'sum',
            'pass_rate': 'mean'
        }).reset_index()
        
        ax1 = axes[0]
        ax2 = ax1.twinx()
        
        bars = ax1.bar(overall_disparity['disparity_range'], overall_disparity['signal_count'], 
                      alpha=0.7, color='lightgreen')
        line = ax2.plot(overall_disparity['disparity_range'], overall_disparity['pass_rate'] * 100, 
                       color='red', marker='o', linewidth=2)
        
        ax1.set_ylabel('ì‹ í˜¸ ê°œìˆ˜', color='green')
        ax2.set_ylabel('í†µê³¼ìœ¨ (%)', color='red')
        ax1.set_title('ğŸ“ˆ ì´ê²©ë„ êµ¬ê°„ë³„ ì‹ í˜¸ ë¶„í¬ ë° í†µê³¼ìœ¨')
        plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45)
        
        # ì „ëµë³„ ì´ê²©ë„ íŒ¨í„´ íˆíŠ¸ë§µ
        pivot_data = disparity_df.pivot(index='strategy_type', columns='disparity_range', values='pass_rate')
        sns.heatmap(pivot_data, annot=True, fmt='.2f', cmap='RdYlGn', ax=axes[1], 
                   vmin=0, vmax=1, cbar_kws={'label': 'í†µê³¼ìœ¨'})
        axes[1].set_title('ğŸ¯ ì „ëµë³„ ì´ê²©ë„ êµ¬ê°„ í†µê³¼ìœ¨')
        axes[1].set_xlabel('ì´ê²©ë„ êµ¬ê°„')
        axes[1].set_ylabel('ì „ëµ')
        
        plt.tight_layout()
        plt.savefig(save_path / 'disparity_patterns.png', dpi=300, bbox_inches='tight')
        plt.close()

    def _plot_time_patterns(self, time_df: pd.DataFrame, save_path: Path):
        """ì‹œê°„ëŒ€ íŒ¨í„´ ì‹œê°í™”"""
        fig, axes = plt.subplots(2, 1, figsize=(15, 10))
        
        # ì‹œê°„ëŒ€ë³„ ì‹ í˜¸ ê°œìˆ˜
        axes[0].bar(time_df['hour'], time_df['signal_count'], color='lightblue', alpha=0.7)
        axes[0].set_xlabel('ì‹œê°„ (24ì‹œê°„)')
        axes[0].set_ylabel('ì‹ í˜¸ ê°œìˆ˜')
        axes[0].set_title('â° ì‹œê°„ëŒ€ë³„ ì‹ í˜¸ ë°œìƒ ë¹ˆë„')
        axes[0].set_xticks(range(0, 24, 2))
        
        # ì‹œê°„ëŒ€ë³„ í†µê³¼ìœ¨ê³¼ í‰ê·  ê°•ë„
        ax1 = axes[1]
        ax2 = ax1.twinx()
        
        line1 = ax1.plot(time_df['hour'], time_df['pass_rate'] * 100, 
                        color='red', marker='o', linewidth=2, label='í†µê³¼ìœ¨')
        line2 = ax2.plot(time_df['hour'], time_df['avg_strength'], 
                        color='blue', marker='s', linewidth=2, label='í‰ê·  ê°•ë„')
        
        ax1.set_xlabel('ì‹œê°„ (24ì‹œê°„)')
        ax1.set_ylabel('í†µê³¼ìœ¨ (%)', color='red')
        ax2.set_ylabel('í‰ê·  ì‹ í˜¸ ê°•ë„', color='blue')
        ax1.set_title('ğŸ“Š ì‹œê°„ëŒ€ë³„ ì‹ í˜¸ í’ˆì§ˆ')
        ax1.set_xticks(range(0, 24, 2))
        
        # ë²”ë¡€
        lines1, labels1 = ax1.get_legend_handles_labels()
        lines2, labels2 = ax2.get_legend_handles_labels()
        ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper right')
        
        plt.tight_layout()
        plt.savefig(save_path / 'time_patterns.png', dpi=300, bbox_inches='tight')
        plt.close()

    def export_ml_dataset(self, output_path: str = "analysis/ml_dataset.csv"):
        """ğŸ¤– ë¨¸ì‹ ëŸ¬ë‹ìš© ë°ì´í„°ì…‹ ë‚´ë³´ë‚´ê¸°"""
        df = self.generate_feature_dataset()
        
        if df.empty:
            print("âš ï¸ ë‚´ë³´ë‚¼ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ê²°ì¸¡ê°’ ì²˜ë¦¬
        numeric_columns = df.select_dtypes(include=[np.number]).columns
        df[numeric_columns] = df[numeric_columns].fillna(df[numeric_columns].median())
        
        # íŒŒì¼ ì €ì¥
        output_file = Path(output_path)
        output_file.parent.mkdir(exist_ok=True, parents=True)
        df.to_csv(output_file, index=False, encoding='utf-8-sig')
        
        print(f"ğŸ“ ML ë°ì´í„°ì…‹ ì €ì¥ ì™„ë£Œ: {output_file}")
        print(f"ğŸ“Š ë°ì´í„° í¬ê¸°: {df.shape}")
        print(f"ğŸ“ˆ Target ë¶„í¬: {df['signal_passed'].value_counts().to_dict()}")

    def print_summary_report(self):
        """ğŸ“‹ ìš”ì•½ ë³´ê³ ì„œ ì¶œë ¥"""
        print("=" * 80)
        print("ğŸ¯ ë¨¸ì‹ ëŸ¬ë‹ ë°ì´í„° ë¶„ì„ ë³´ê³ ì„œ")
        print("=" * 80)
        
        # ê¸°ë³¸ í†µê³„
        stats = self.get_basic_stats()
        print(f"\nğŸ“Š ê¸°ë³¸ í†µê³„:")
        print(f"  â€¢ ì´ ì‹ í˜¸ ìˆ˜: {stats['signals']['total_signals']:,}ê°œ")
        print(f"  â€¢ ì‹ í˜¸ í†µê³¼ìœ¨: {stats['signal_pass_rate']:.1%}")
        print(f"  â€¢ ë§¤ìˆ˜ ì„±ê³µë¥ : {stats['buy_success_rate']:.1%}")
        print(f"  â€¢ ê³ ìœ  ì¢…ëª© ìˆ˜: {stats['signals']['unique_stocks']:,}ê°œ")
        print(f"  â€¢ ê³ ìœ  ì „ëµ ìˆ˜: {stats['signals']['unique_strategies']:,}ê°œ")
        
        # ì£¼ìš” ì‹¤íŒ¨ ì›ì¸
        failure_analysis = self.analyze_signal_failures()
        if not failure_analysis.empty:
            print(f"\nğŸš« ì£¼ìš” ì‹¤íŒ¨ ì›ì¸ Top 5:")
            top_failures = failure_analysis.groupby('signal_reason')['failure_count'].sum().nlargest(5)
            for i, (reason, count) in enumerate(top_failures.items(), 1):
                print(f"  {i}. {reason}: {count:,}íšŒ")
        
        # ì´ê²©ë„ ë¶„ì„
        disparity_patterns = self.analyze_disparity_patterns()
        if not disparity_patterns.empty:
            print(f"\nğŸ“ˆ ì´ê²©ë„ êµ¬ê°„ë³„ í†µê³¼ìœ¨:")
            overall_disparity = disparity_patterns.groupby('disparity_range').agg({
                'signal_count': 'sum',
                'pass_rate': 'mean'
            }).round(3)
            for range_name, row in overall_disparity.iterrows():
                print(f"  â€¢ {range_name}: {row['pass_rate']:.1%} ({row['signal_count']:,}ê°œ)")
        
        print("\n" + "=" * 80)

    def analyze_enhanced_patterns(self) -> pd.DataFrame:
        """ğŸ†• í™•ì¥ëœ íŒ¨í„´ ë¶„ì„"""
        with sqlite3.connect(str(self.db_path)) as conn:
            query = """
                SELECT 
                    strategy_type,
                    CASE 
                        WHEN hour_of_day BETWEEN 9 AND 11 THEN 'ì˜¤ì „'
                        WHEN hour_of_day BETWEEN 12 AND 14 THEN 'ì˜¤í›„' 
                        ELSE 'ê¸°íƒ€'
                    END as time_period,
                    CASE 
                        WHEN disparity_20d <= 90 THEN 'ê³¼ë§¤ë„'
                        WHEN disparity_20d <= 110 THEN 'ì¤‘ë¦½'
                        ELSE 'ê³¼ë§¤ìˆ˜'
                    END as disparity_zone,
                    CASE 
                        WHEN volume_ratio_20d >= 2.0 THEN 'ê³ ê±°ë˜ëŸ‰'
                        WHEN volume_ratio_20d >= 1.5 THEN 'ì¤‘ê±°ë˜ëŸ‰'
                        ELSE 'ì €ê±°ë˜ëŸ‰'
                    END as volume_zone,
                    COUNT(*) as signal_count,
                    AVG(CASE WHEN signal_passed = 1 THEN 1.0 ELSE 0.0 END) as pass_rate,
                    AVG(signal_strength) as avg_strength,
                    AVG(rsi) as avg_rsi,
                    AVG(volatility_20d) as avg_volatility
                FROM signal_analysis 
                WHERE signal_strength IS NOT NULL
                GROUP BY strategy_type, time_period, disparity_zone, volume_zone
                HAVING signal_count >= 5
                ORDER BY pass_rate DESC, signal_count DESC
            """
            return pd.read_sql_query(query, conn)

    def analyze_market_conditions_impact(self) -> pd.DataFrame:
        """ğŸ†• ì‹œì¥ ìƒí™©ë³„ ì„±ê³¼ ë¶„ì„"""
        with sqlite3.connect(str(self.db_path)) as conn:
            query = """
                SELECT 
                    CASE 
                        WHEN market_volatility <= 0.1 THEN 'ì•ˆì •'
                        WHEN market_volatility <= 0.2 THEN 'ë³´í†µ'
                        ELSE 'ê³ ë³€ë™'
                    END as volatility_regime,
                    CASE 
                        WHEN kospi_change_pct >= 1.0 THEN 'ê°•ì„¸ì¥'
                        WHEN kospi_change_pct >= -1.0 THEN 'ë³´í•©ì¥'
                        ELSE 'ì•½ì„¸ì¥'
                    END as market_trend,
                    strategy_type,
                    COUNT(*) as attempt_count,
                    AVG(CASE WHEN attempt_result = 'SUCCESS' THEN 1.0 ELSE 0.0 END) as success_rate,
                    AVG(signal_strength) as avg_signal_strength
                FROM buy_attempts ba
                WHERE kospi_change_pct IS NOT NULL AND market_volatility IS NOT NULL
                GROUP BY volatility_regime, market_trend, strategy_type
                HAVING attempt_count >= 3
                ORDER BY success_rate DESC
            """
            return pd.read_sql_query(query, conn)

    def get_feature_importance_data(self) -> pd.DataFrame:
        """ğŸ†• í”¼ì²˜ ì¤‘ìš”ë„ ë¶„ì„ìš© ë°ì´í„°"""
        with sqlite3.connect(str(self.db_path)) as conn:
            query = """
                SELECT 
                    signal_strength, disparity_20d, volume_ratio_20d, rsi, macd,
                    bb_position, volatility_20d, momentum_20d, hour_of_day,
                    CASE WHEN day_of_week IN (0,1,2,3,4) THEN 1 ELSE 0 END as is_weekday,
                    CASE WHEN is_opening_hour = 1 THEN 1 ELSE 0 END as is_opening,
                    CASE WHEN is_closing_hour = 1 THEN 1 ELSE 0 END as is_closing,
                    signal_passed as target
                FROM signal_analysis 
                WHERE signal_strength IS NOT NULL 
                AND disparity_20d IS NOT NULL 
                AND volume_ratio_20d IS NOT NULL
            """
            return pd.read_sql_query(query, conn)


if __name__ == "__main__":
    # ì‚¬ìš© ì˜ˆì‹œ
    try:
        analyzer = MLDataAnalyzer()
        
        # ìš”ì•½ ë³´ê³ ì„œ ì¶œë ¥
        analyzer.print_summary_report()
        
        # ì‹œê°í™” ìƒì„±
        analyzer.create_visualizations()
        
        # ë¨¸ì‹ ëŸ¬ë‹ ë°ì´í„°ì…‹ ë‚´ë³´ë‚´ê¸°
        analyzer.export_ml_dataset()
        
        print("\nâœ… ë¶„ì„ ì™„ë£Œ!")
        
    except FileNotFoundError as e:
        print(f"âŒ ì˜¤ë¥˜: {e}")
        print("ğŸ’¡ íŒíŠ¸: ì‹œìŠ¤í…œì„ ì‹¤í–‰í•˜ì—¬ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
    except Exception as e:
        print(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}") 